{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training model on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from sklearn.metrics import auc\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "from six.moves import xrange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from cifar import cifar10_utils\n",
    "\n",
    "from Utils import utils\n",
    "from Models.rfnn import RFNN\n",
    "from Models.ctnet import CTNET\n",
    "\n",
    "NUM_GPUS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_folders():\n",
    "    \"\"\"\n",
    "    Initializes all folders in FLAGS variable.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    else:\n",
    "        shutil.rmtree(FLAGS.log_dir)\n",
    "        tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "\n",
    "    # if not tf.gfile.Exists(FLAGS.checkpoint_dir):\n",
    "    # tf.gfile.MakeDirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "\n",
    "def print_flags():\n",
    "    \"\"\"\n",
    "    Prints all entries in FLAGS variable.\n",
    "    \"\"\"\n",
    "    for key, value in vars(FLAGS).items():\n",
    "        print(key + ' : ' + str(value))\n",
    "\n",
    "\n",
    "def str2bool(s):\n",
    "    if s == \"True\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_experiment_infos():\n",
    "    l = [\n",
    "        \"learning_rate: \" + str(FLAGS.learning_rate),\n",
    "        \"max_epochs: \" + str(FLAGS.max_epochs),\n",
    "        \"batch_size: \" + str(FLAGS.batch_size),\n",
    "        \"pretraining: \" + str(FLAGS.pretraining),\n",
    "        \"xvalidation_folds: \" + str(FLAGS.xvalidation_folds),\n",
    "        \"normalization: \" + str(FLAGS.normalization),\n",
    "        \"batch_normalization: \" + str(FLAGS.batch_normalization),\n",
    "        \"sigmas: \" + str(FLAGS.sigmas),\n",
    "        \"kernels: \" + str(FLAGS.kernels),\n",
    "        \"maps: \" + str(FLAGS.maps),\n",
    "        \"bases: \" + str(FLAGS.bases),\n",
    "        \"bases3d: \" + str(FLAGS.bases3d),\n",
    "        \"print_freq: \" + str(FLAGS.print_freq),\n",
    "        \"eval_freq: \" + str(FLAGS.eval_freq),\n",
    "        \"log_dir: \" + str(FLAGS.log_dir),\n",
    "        \"trainingpath: \" + str(FLAGS.trainingpath),\n",
    "        \"testpath: \" + str(FLAGS.testpath),\n",
    "    ]\n",
    "    return tf.convert_to_tensor(l)\n",
    "\n",
    "def get_kernels(i):\n",
    "    kernel = tf.get_default_graph().get_tensor_by_name(\"tower_0/ConvLayer%d/weights_0:0\" % i)\n",
    "    alphas = tf.get_default_graph().get_tensor_by_name(\"ConvLayer%d/alphas:0\" % i)\n",
    "#    print(kernel.get_shape())\n",
    "    \n",
    "    kernel_avg = tf.reduce_mean(kernel, axis=2)\n",
    "#    print(kernel_avg.get_shape())\n",
    "    x_min = tf.reduce_min(kernel_avg)\n",
    "    x_max = tf.reduce_max(kernel_avg)\n",
    "    kernel_0_to_1 = (kernel_avg - x_min) / (x_max - x_min)\n",
    "\n",
    "    # to tf.image_summary format [batch_size, height, width, channels]\n",
    "    kernel_transposed = tf.transpose(kernel_avg, [2, 0, 1])\n",
    "    # print(kernel_transposed.get_shape())\n",
    "\n",
    "    return alphas, kernel_transposed\n",
    "\n",
    "def show_kernels(kernels):\n",
    "    f, axarr = plt.subplots(8, 8)\n",
    "    f.set_figheight(12)\n",
    "    f.set_figwidth(12)\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            axarr[j, i].imshow(kernels[i*3+j], cmap='gray')\n",
    "            axarr[j, i].set_axis_off()\n",
    "\n",
    "def exp_GB(logits, alpha):\n",
    "    return tf.multiply(tf.exp(logits), alpha)\n",
    "\n",
    "def pow3_GB(logits, alpha, beta):\n",
    "    return tf.add(tf.multiply(tf.pow(logits, 3), alpha), beta)\n",
    "\n",
    "def tower_accuracy(logits, labels, scope):\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(softmax, 1), tf.argmax(labels, 1))\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#        tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def tower_accuracy_exp(logits, labels, scope):\n",
    "#    softmax = tf.nn.softmax(logits)\n",
    "#    signal = exp_GB(logits, FLAGS.alpha)\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#        tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "    return accuracy, correct_prediction, logits\n",
    "\n",
    "\n",
    "def tower_loss(logits, labels, scope):\n",
    "    with tf.name_scope('Cross_Entropy_Loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    with tf.name_scope('Total_Loss'):\n",
    "        total_loss = tf.add_n(tf.get_collection('losses', scope), name='total_loss')\n",
    "#        tf.summary.scalar('Total_loss', total_loss)\n",
    "    return total_loss\n",
    "\n",
    "def tower_loss_exp(logits, labels, scope):\n",
    "    with tf.name_scope('Cross_Entropy_Loss'):\n",
    "#        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "#        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        signal = exp_GB(logits, FLAGS.alpha)\n",
    "        sqdiff = tf.squared_difference(signal, labels)\n",
    "        mean_rms = tf.reduce_mean(sqdiff)\n",
    "        tf.add_to_collection('losses', mean_rms)\n",
    "    with tf.name_scope('Total_Loss'):\n",
    "        total_loss = tf.add_n(tf.get_collection('losses', scope), name='total_loss')\n",
    "#        tf.summary.scalar('Total_loss', total_loss)\n",
    "    return total_loss, sqdiff\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, v in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "\n",
    "    return average_grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cifar():\n",
    "    # Set the random seeds for reproducibility. DO NOT CHANGE.\n",
    "    tf.set_random_seed(42)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "            # ====== LOAD DATASET ======\n",
    "            print('Loading Dataset...')\n",
    "            cifar10_dataset = cifar10_utils.get_cifar10('/home/nicolab/Downloads/cifar-10-batches-py')\n",
    "            print('Loading Dataset...done.')\n",
    "\n",
    "            # ====== DEFINE SPACEHOLDERS ======\n",
    "            with tf.name_scope('input'):\n",
    "                image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 32, 32, 3], name='x-input')\n",
    "                label_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 10], name='y-input')\n",
    "                is_training = tf.placeholder(tf.bool, name='is-training')\n",
    "\n",
    "\n",
    "            # ====== DEFINE FEED_DICTIONARY ======\n",
    "            def feed_dict(flag):\n",
    "                xs = []\n",
    "                ys = []\n",
    "                if flag == 0:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = cifar10_dataset.train.next_batch(FLAGS.batch_size)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                elif flag == 1:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = cifar10_dataset.test.next_batch(FLAGS.batch_size)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                return {image_batch: xs, label_batch: ys, is_training: flag == 0}\n",
    "\n",
    "\n",
    "            # ====== MODEL DEFINITION ======\n",
    "            print('Defining model...')\n",
    "\n",
    "            sigmas = [float(x) for x in FLAGS.sigmas.split(',')]\n",
    "            kernels = [int(x) for x in FLAGS.kernels.split(',')]\n",
    "            maps = [int(x) for x in FLAGS.maps.split(',')]\n",
    "            bases = [int(x) for x in FLAGS.bases.split(',')]\n",
    "            strides = [int(x) for x in FLAGS.strides.split(',')]\n",
    "\n",
    "#            model = RFNN(\n",
    "#                n_classes=10,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                sigmas=sigmas,\n",
    "#                bases=bases,\n",
    "#                bases_3d=FLAGS.bases3d,\n",
    "#                is_training=is_training,\n",
    "#                batchnorm=FLAGS.batch_normalization\n",
    "#            )\n",
    "            model = CTNET(\n",
    "                n_classes=10,\n",
    "                kernels=kernels,\n",
    "                maps=maps,\n",
    "                strides=strides,\n",
    "                pretraining=False,\n",
    "                is_training = is_training, \n",
    "                conv3d=False,\n",
    "                bnorm=FLAGS.batch_normalization)\n",
    "\n",
    "            print('Defining model...done.')\n",
    "\n",
    "            # ====== DEFINE LOSS, ACCURACY TENSORS ======\n",
    "            print('Defining necessary OPs...')\n",
    "\n",
    "            opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "\n",
    "            # === DEFINE QUEUE OPS ===\n",
    "            batch_queue = tf.FIFOQueue(\n",
    "                    capacity=NUM_GPUS,\n",
    "                    dtypes=[tf.float32, tf.float32],\n",
    "                    shapes=[ (FLAGS.batch_size,32,32,3), (FLAGS.batch_size,10) ]\n",
    "                )\n",
    "            batch_enqueue = batch_queue.enqueue_many([image_batch, label_batch])\t\t\n",
    "            close_queue = batch_queue.close()\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        tower_losses = []\n",
    "        tower_accuracies = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in xrange(NUM_GPUS):\n",
    "\n",
    "                x, y = batch_queue.dequeue()\n",
    "\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                        # ====== INFERENCE ======\n",
    "                        if FLAGS.pretraining:\n",
    "                            print('Pre-training model...')\n",
    "\n",
    "                            network_architecture = \\\n",
    "                                {\n",
    "                                    'Conv_kernels': kernels,\n",
    "                                    'Conv_maps': maps\n",
    "                                }\n",
    "                            ae = Autoencoder(network_architecture)\n",
    "\n",
    "                            assign_ops, net = ae.load_weights(x, FLAGS.pretrained_weights_path, FLAGS.pretrained_biases_path, is_training)\n",
    "\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(net)\n",
    "\n",
    "                            print('Pre-training model...done.')\n",
    "                        else:\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(x)\n",
    "\n",
    "                        loss, sqdifference = tower_loss_exp(logits, y, scope)\n",
    "                        accuracy,correct_prediction,signal = tower_accuracy_exp(logits, y, scope)\n",
    "                        exp_signal = exp_GB(signal, FLAGS.alpha)\n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                        # Retain the summaries from the final tower.\n",
    "                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                        # Calculate the gradients for the batch of data on this tower.\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "\n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_accuracies.append(accuracy)\n",
    "        with tf.device('/cpu:0'):\n",
    "\n",
    "            # Calculate the mean of each gradient - synchronization point across towers.\n",
    "            avg_grads = average_gradients(tower_grads)\n",
    "            avg_loss = tf.reduce_mean(tower_losses, 0)\n",
    "            avg_accuracy = tf.reduce_mean(tower_accuracies, 0)\n",
    "\n",
    "            print('Defining necessary OPs...done.')\n",
    "\n",
    "            # ====== ADD SUMMARIES ======\n",
    "\n",
    "            # Gradients\n",
    "            for grad, var in avg_grads:\n",
    "                if grad is not None:\n",
    "                    summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\t\n",
    "\n",
    "            # Trainable variables\n",
    "            for var in tf.trainable_variables():\n",
    "                summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "\n",
    "            # Print initial kernels\n",
    "            # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "            # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "            # np.save('./Kernels/kernel_0.npy', kernels_array)\n",
    "            # np.save('./Kernels/alphas_0.npy', alphas)\n",
    "\n",
    "            # ====== UPDATE VARIABLES ======\n",
    "\n",
    "            print('Defining update OPs...')\n",
    "\n",
    "            # Apply the gradients to adjust the shared variables.\n",
    "            apply_gradient_op = opt.apply_gradients(avg_grads, global_step=global_step)\n",
    "\n",
    "            # Track the moving averages of all trainable variables.\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "                        0.9999, global_step)\n",
    "            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "            # Group all updates into a single train op.\n",
    "            train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "            print('Defining update OPs...done.')\n",
    "\n",
    "            # ====== SAVING OPS ======\n",
    "\n",
    "            # Create a saver.\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            # Build the summary operation from the last tower summaries.\n",
    "            summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "            # ====== DEFINE SESSION AND OPTIMIZE ======\n",
    "            config = tf.ConfigProto(allow_soft_placement=True)\n",
    "            config.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "                \n",
    "            print('Training model...')\n",
    "            for f in range(1):\n",
    "                try:\n",
    "                    training_steps = int(cifar10_dataset.train.num_examples / (NUM_GPUS*FLAGS.batch_size))\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#                    print([n.name for n in tf.get_default_graph().as_graph_def().node\n",
    "#                               if 'ConvLayer1' in n.name and 'alphas' in n.name])\n",
    "                    \n",
    "                    # Show initial kernels from 1st layer\n",
    "#                    alphas_t, kernels_t = get_kernels(1)\n",
    "#                    alphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "#                    show_kernels(kernels)\n",
    "                    \n",
    "                    # Create a coordinator, launch the queue runner threads.\n",
    "                    coord = tf.train.Coordinator()\n",
    "                    tf.train.start_queue_runners(sess, coord=coord)\n",
    "\n",
    "                    # Assign ops if pre-training\n",
    "                    if FLAGS.pretraining:\n",
    "                        for assign_op in assign_ops:\n",
    "                            sess.run(assign_op)\n",
    "\n",
    "                    # Init writers\n",
    "                    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train/' + str(f), sess.graph)\n",
    "                    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test/'  + str(f))\n",
    "                    \n",
    "                    max_acc = 0\n",
    "                    for i in range(int(FLAGS.max_epochs * training_steps)):\n",
    "\n",
    "                        if coord.should_stop():\n",
    "                            break\n",
    "\n",
    "                        # ------------ TRAIN -------------\n",
    "                        if i % (FLAGS.print_freq * training_steps) == 0:\n",
    "                            # ------------ PRINT -------------\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, summaries, loss_value, acc_value, logits, sqdiff = \\\n",
    "                                    sess.run([train_op, summary_op, avg_loss, avg_accuracy, signal, sqdifference], feed_dict={is_training: True})\n",
    "                            print(logits)\n",
    "                            print(sqdiff)\n",
    "                            print(loss_value)\n",
    "                            print(acc_value)\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=acc_value)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=loss_value)\n",
    "\n",
    "                            train_writer.add_summary(summaries, i)\n",
    "                            train_writer.add_summary(summary, i)\n",
    "                        else:\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, loss_value = sess.run([train_op, avg_loss], feed_dict={is_training: True})\n",
    "\n",
    "                        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "\n",
    "                        if i % (FLAGS.eval_freq * training_steps) == 0 or i == int(FLAGS.max_epochs * training_steps):\n",
    "                            # ------------ VALIDATON -------------\n",
    "                            validation_steps = int(cifar10_dataset.test.num_examples / (FLAGS.batch_size*NUM_GPUS))\n",
    "                            tot_acc = 0.0\n",
    "                            tot_loss = 0.0\t\t\t\t\n",
    "                            for step in range(validation_steps):\n",
    "                                sess.run(batch_enqueue, feed_dict=feed_dict(1))\n",
    "                                acc_s, loss_s, sm, y_ = sess.run([avg_accuracy, avg_loss, correct_prediction, y], feed_dict={is_training: False})\n",
    "#                                loss_s = sess.run(avg_loss, feed_dict={is_training: False})\n",
    "                                tot_acc += (acc_s / validation_steps)\n",
    "                                tot_loss += (loss_s / validation_steps)\n",
    "        \n",
    "#                                print(sm)\n",
    "#                                print(y_)\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=tot_acc)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=tot_loss)\n",
    "\n",
    "                            test_writer.add_summary(summary, i)\n",
    "\n",
    "    #\t\t\t\t\t\tif tot_acc > max_acc:\n",
    "    #\t\t\t\t\t\t\tmax_acc = tot_acc\n",
    "    #\t\t\t\t\t\tprint('Validation accuracy at step %s: %s' % (i, tot_acc))\n",
    "                            print('Validation loss at step %s: %s' % (i, tot_loss))\n",
    "\n",
    "\n",
    "                        # if i % FLAGS.checkpoint_freq == 0: # or i == FLAGS.max_steps:\n",
    "                        # checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n",
    "                        # saver.save(sess, checkpoint_path, global_step=i)\n",
    "\n",
    "                    train_writer.close()\n",
    "                    test_writer.close()\n",
    "                    print('Max validation accuracy in fold %s: %s' % (f,max_acc))\n",
    "                    \n",
    "                    # Show final kernels from 1st layer\n",
    "#                    alphas_t, kernels_t = get_kernels(1)\n",
    "#                    alphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "#                    show_kernels(kernels)\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Report exceptions to the coordinator.\n",
    "                    coord.request_stop(e)\n",
    "                finally:\n",
    "                    # Terminate threads\n",
    "                    coord.request_stop()\n",
    "                    coord.join()\n",
    "            sess.run(close_queue)\n",
    "\n",
    "\n",
    "    # Print final kernels\n",
    "    # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "    # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "    # np.save('./Kernels/kernel_final.npy', kernels_array)\n",
    "    # np.save('./Kernels/alphas_final.npy', alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ctnet():\n",
    "    # Set the random seeds for reproducibility. DO NOT CHANGE.\n",
    "    tf.set_random_seed(42)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "            # ====== LOAD DATASET ======\n",
    "            print('Loading Dataset...')\n",
    "            with open(FLAGS.trainingpath, 'rb') as handle:\n",
    "                training_points = pickle.load(handle)\n",
    "            with open(FLAGS.testpath, 'rb') as handle:\n",
    "                test_points = pickle.load(handle)\n",
    "\n",
    "            dataset = utils.DataSet(np.array(list(training_points.keys())), np.array(list(training_points.values())),\n",
    "                    np.array(list(test_points.keys())), np.array(list(test_points.values())),\n",
    "                    cross_validation_folds=FLAGS.xvalidation_folds,\n",
    "                    normalize = FLAGS.normalization)\n",
    "            print('Loading Dataset...done.')\n",
    "\n",
    "            # ====== DEFINE SPACEHOLDERS ======\n",
    "            with tf.name_scope('input'):\n",
    "                if FLAGS.bases3d:\n",
    "                    image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 512, 512, 30, 1], name='x-input')\n",
    "                else:\n",
    "                    image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 512, 512, 30], name='x-input')\n",
    "                label_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 2], name='y-input')\n",
    "                is_training = tf.placeholder(tf.bool, name='is-training')\n",
    "\n",
    "\n",
    "            # ====== DEFINE FEED_DICTIONARY ======\n",
    "            def feed_dict(flag):\n",
    "                xs = []\n",
    "                ys = []\n",
    "                if flag == 0:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = dataset.Training.next_batch(FLAGS.batch_size, bases3d=FLAGS.bases3d)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                elif flag == 1:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = dataset.Validation.next_batch(FLAGS.batch_size, bases3d=FLAGS.bases3d)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                return {image_batch: xs, label_batch: ys, is_training: flag == 0}\n",
    "\n",
    "\n",
    "            # ====== MODEL DEFINITION ======\n",
    "            print('Defining model...')\n",
    "\n",
    "            sigmas = [float(x) for x in FLAGS.sigmas.split(',')]\n",
    "            kernels = [int(x) for x in FLAGS.kernels.split(',')]\n",
    "            maps = [int(x) for x in FLAGS.maps.split(',')]\n",
    "            bases = [int(x) for x in FLAGS.bases.split(',')]\n",
    "            strides = [int(x) for x in FLAGS.strides.split(',')]\n",
    "\n",
    "#            model = RFNN(\n",
    "#                n_classes=2,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                sigmas=sigmas,\n",
    "#                bases=bases,\n",
    "#                bases_3d=FLAGS.bases3d,\n",
    "#                is_training=is_training,\n",
    "#                batchnorm=FLAGS.batch_normalization\n",
    "#            )\n",
    "            model = CTNET(\n",
    "                n_classes=2,\n",
    "                kernels=kernels,\n",
    "                maps=maps,\n",
    "                strides=strides,\n",
    "                pretraining=False,\n",
    "                is_training = is_training, \n",
    "                conv3d=FLAGS.bases3d,\n",
    "                bnorm=FLAGS.batch_normalization)\n",
    "\n",
    "            print('Defining model...done.')\n",
    "\n",
    "            # ====== DEFINE LOSS, ACCURACY TENSORS ======\n",
    "            print('Defining necessary OPs...')\n",
    "\n",
    "            opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "\n",
    "\n",
    "            # === DEFINE QUEUE OPS ===\n",
    "            batch_queue = tf.FIFOQueue(\n",
    "                capacity=NUM_GPUS,\n",
    "                dtypes=[tf.float32, tf.float32],\n",
    "                shapes=[ (FLAGS.batch_size,512,512,30,1) if FLAGS.bases3d else (FLAGS.batch_size,512,512,30),\n",
    "                             (FLAGS.batch_size,2) ]\n",
    "            )\n",
    "            batch_enqueue = batch_queue.enqueue_many([image_batch, label_batch])\t\t\n",
    "            close_queue = batch_queue.close()\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        tower_losses = []\n",
    "        tower_accuracies = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in xrange(NUM_GPUS):\n",
    "\n",
    "                x, y = batch_queue.dequeue()\n",
    "\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                        # ====== INFERENCE ======\n",
    "                        if FLAGS.pretraining:\n",
    "                            print('Pre-training model...')\n",
    "\n",
    "                            network_architecture = \\\n",
    "                            {\n",
    "                            'Conv_kernels': kernels,\n",
    "                            'Conv_maps': maps\n",
    "                            }\n",
    "                            ae = Autoencoder(network_architecture)\n",
    "\n",
    "                            assign_ops, net = ae.load_weights(x, FLAGS.pretrained_weights_path, FLAGS.pretrained_biases_path, is_training)\n",
    "\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(net)\n",
    "\n",
    "                            print('Pre-training model...done.')\n",
    "                        else:\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(x)\n",
    "\n",
    "                        loss = tower_loss(logits, y, scope)\n",
    "                        accuracy = tower_accuracy(logits, y, scope)\n",
    "\n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                        # Retain the summaries from the final tower.\n",
    "                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                        # Calculate the gradients for the batch of data on this tower.\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "\n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_accuracies.append(accuracy)\n",
    "        with tf.device('/cpu:0'):\n",
    "\n",
    "            # Calculate the mean of each gradient - synchronization point across towers.\n",
    "            avg_grads = average_gradients(tower_grads)\n",
    "            avg_loss = tf.reduce_mean(tower_losses, 0)\n",
    "            avg_accuracy = tf.reduce_mean(tower_accuracies, 0)\n",
    "\n",
    "            print('Defining necessary OPs...done.')\n",
    "\n",
    "            # ====== ADD SUMMARIES ======\n",
    "\n",
    "            # Gradients\n",
    "            for grad, var in avg_grads:\n",
    "                if grad is not None:\n",
    "                        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\t\n",
    "\n",
    "            # Trainable variables\n",
    "            for var in tf.trainable_variables():\n",
    "                summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "\n",
    "            # Print initial kernels\n",
    "            # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "            # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "            # np.save('./Kernels/kernel_0.npy', kernels_array)\n",
    "            # np.save('./Kernels/alphas_0.npy', alphas)\n",
    "\n",
    "            # ====== UPDATE VARIABLES ======\n",
    "\n",
    "            print('Defining update OPs...')\n",
    "\n",
    "            # Apply the gradients to adjust the shared variables.\n",
    "            apply_gradient_op = opt.apply_gradients(avg_grads, global_step=global_step)\n",
    "\n",
    "            # Track the moving averages of all trainable variables.\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "                0.9999, global_step)\n",
    "            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "            # Group all updates into a single train op.\n",
    "            train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "            print('Defining update OPs...done.')\n",
    "\n",
    "            # ====== SAVING OPS ======\n",
    "\n",
    "            # Create a saver.\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            # Build the summary operation from the last tower summaries.\n",
    "            summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "            # ====== DEFINE SESSION AND OPTIMIZE ======\n",
    "            config = tf.ConfigProto(allow_soft_placement=True)\n",
    "            config.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            print('Training model...')\n",
    "\n",
    "            for f in range(1):\n",
    "\n",
    "                # Create a coordinator, launch the queue runner threads.\n",
    "                coord = tf.train.Coordinator()\n",
    "                tf.train.start_queue_runners(sess, coord=coord)\n",
    "\n",
    "                try:\n",
    "                    training_steps = int(dataset.Training.num_examples / (NUM_GPUS*FLAGS.batch_size))\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            #                    \tprint([n.name for n in tf.get_default_graph().as_graph_def().node\n",
    "            #                               if 'ConvLayer1' in n.name and 'alphas' in n.name])\n",
    "\n",
    "                    # Show initial kernels from 1st layer\n",
    "    #\t\t\t\talphas_t, kernels_t = get_kernels(1)\n",
    "    #\t\t\t\talphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "    #\t\t\t\tshow_kernels(kernels)\n",
    "\n",
    "                    # Assign ops if pre-training\n",
    "                    if FLAGS.pretraining:\n",
    "                        for assign_op in assign_ops:\n",
    "                            sess.run(assign_op)\n",
    "\n",
    "                    # Init writers\n",
    "                    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train/' + str(f), sess.graph)\n",
    "                    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test/'  + str(f))\n",
    "\n",
    "                    max_acc = 0\n",
    "                    for i in range(int(FLAGS.max_epochs * training_steps)):\n",
    "\n",
    "                        if coord.should_stop():\n",
    "                            break\n",
    "\n",
    "                        # ------------ TRAIN -------------\n",
    "                        if i % (FLAGS.print_freq * training_steps) == 0:\n",
    "                            # ------------ PRINT -------------\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, summaries, loss_value, acc_value = sess.run([train_op, summary_op, avg_loss, avg_accuracy], feed_dict={is_training: True})\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=acc_value)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=loss_value)\n",
    "\n",
    "                            train_writer.add_summary(summaries, i)\n",
    "                            train_writer.add_summary(summary, i)\n",
    "                        else:\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, loss_value = sess.run([train_op, avg_loss], feed_dict={is_training: True})\n",
    "\n",
    "                        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "                        if i % (FLAGS.eval_freq * training_steps) == 0 or i == int(FLAGS.max_epochs * training_steps):\n",
    "                            # ------------ VALIDATON -------------\n",
    "                            validation_steps = int(dataset.Validation.num_examples / (FLAGS.batch_size*NUM_GPUS))\n",
    "                            tot_acc = 0.0\n",
    "                            tot_loss = 0.0\t\t\t\t\n",
    "                            for step in range(validation_steps):\n",
    "                                sess.run(batch_enqueue, feed_dict=feed_dict(1))\n",
    "                                acc_s, loss_s = sess.run([avg_accuracy, avg_loss], feed_dict={is_training: False})\n",
    "                                tot_acc += (acc_s / validation_steps)\n",
    "                                tot_loss += (loss_s / validation_steps)\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=tot_acc)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=tot_loss)\n",
    "\n",
    "                            test_writer.add_summary(summary, i)\n",
    "\n",
    "                            if tot_acc > max_acc:\n",
    "                                max_acc = tot_acc\n",
    "                            print('Validation loss at step %s: %s' % (i, tot_loss))\n",
    "\n",
    "\n",
    "                        # if i % FLAGS.checkpoint_freq == 0: # or i == FLAGS.max_steps:\n",
    "                        # checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n",
    "                        # saver.save(sess, checkpoint_path, global_step=i)\n",
    "\n",
    "                    train_writer.close()\n",
    "                    test_writer.close()\n",
    "                    print('Max validation accuracy in fold %s: %s' % (f,max_acc))\n",
    "\n",
    "                    # Show final kernels from 1st layer\n",
    "    #\t\t\t\talphas_t, kernels_t = get_kernels(1)\n",
    "    #\t\t\t\talphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "    #\t\t\t\tshow_kernels(kernels)\n",
    "\n",
    "    #\t\t\tdataset.next_fold()\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Report exceptions to the coordinator.\n",
    "                    coord.request_stop(e)\n",
    "                finally:\n",
    "                    # Terminate threads\n",
    "                    coord.request_stop()\n",
    "                    coord.join()\n",
    "            sess.run(close_queue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "FLAGS = FLAGS()\n",
    "\n",
    "FLAGS.learning_rate = 0.01\n",
    "FLAGS.max_epochs = 70\n",
    "FLAGS.batch_size = 8\n",
    "FLAGS.pretraining = False\n",
    "FLAGS.xvalidation_folds = 8\n",
    "FLAGS.normalization = False\n",
    "FLAGS.batch_normalization = False\n",
    "FLAGS.alpha = 0.1\n",
    "\n",
    "FLAGS.sigmas = '1.5'\n",
    "FLAGS.kernels = '11,5,5'\n",
    "FLAGS.strides = '2,1,2'\n",
    "FLAGS.maps = '32,32'\n",
    "FLAGS.bases = '20,20,20'\n",
    "FLAGS.bases3d = False\n",
    "\n",
    "FLAGS.print_freq = 1\n",
    "FLAGS.eval_freq = 1\n",
    "FLAGS.checkpoint_freq = 0\n",
    "\n",
    "FLAGS.log_dir = '/home/nicolab/DATA/logs/CTNET/Skull/' + str(FLAGS.learning_rate)\n",
    "FLAGS.trainingpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_SKULLSTRIPPED_RESAMPLED/training_points.npy'\n",
    "FLAGS.testpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_SKULLSTRIPPED_RESAMPLED/test_points.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Init Dataset...\n",
      "Creating folds...\n",
      "Creating folds...done.\n",
      "Init Dataset...done.\n",
      "Loading Dataset...done.\n",
      "Defining model...\n",
      "Defining model...done.\n",
      "Defining necessary OPs...\n",
      "(8, 512, 512, 30)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 128, 128, 2)\n",
      "(8, 1, 1, 2)\n",
      "(8, 2)\n",
      "(8, 512, 512, 30)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 128, 128, 2)\n",
      "(8, 1, 1, 2)\n",
      "(8, 2)\n",
      "(8, 512, 512, 30)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 128, 128, 2)\n",
      "(8, 1, 1, 2)\n",
      "(8, 2)\n",
      "(8, 512, 512, 30)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 256, 256, 32)\n",
      "INFO:tensorflow:Summary name /filters is illegal; using filters instead.\n",
      "(8, 128, 128, 2)\n",
      "(8, 1, 1, 2)\n",
      "(8, 2)\n",
      "Defining necessary OPs...done.\n",
      "Defining update OPs...\n",
      "Defining update OPs...done.\n",
      "Training model...\n",
      "Validation loss at step 0: 0.693535546462\n",
      "Validation loss at step 23: 0.692972580592\n",
      "Validation loss at step 46: 0.69236133496\n",
      "Validation loss at step 69: 0.692897260189\n",
      "Validation loss at step 92: 0.693265080452\n",
      "Validation loss at step 115: 0.693426311016\n",
      "Validation loss at step 138: 0.694571812948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3f6d2408a059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minitialize_folders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_ctnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-96dd5ec7132a>\u001b[0m in \u001b[0;36mtrain_ctnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m                             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_enqueue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m                             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initialize_folders()\n",
    "train_ctnet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
