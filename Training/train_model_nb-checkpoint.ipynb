{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training model on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from sklearn.metrics import auc\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "from six.moves import xrange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from cifar import cifar10_utils\n",
    "\n",
    "from Utils import utils\n",
    "from Models.rfnn import RFNN\n",
    "from Models.ctnet import CTNET\n",
    "from Models.densenet import DenseNet\n",
    "\n",
    "NUM_GPUS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_folders():\n",
    "    \"\"\"\n",
    "    Initializes all folders in FLAGS variable.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    else:\n",
    "        shutil.rmtree(FLAGS.log_dir, ignore_errors=True)\n",
    "        tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "\n",
    "    if not tf.gfile.Exists(FLAGS.checkpoint_dir):\n",
    "        tf.gfile.MakeDirs(FLAGS.checkpoint_dir)\n",
    "    else:\n",
    "        shutil.rmtree(FLAGS.checkpoint_dir, ignore_errors=True)\n",
    "        tf.gfile.MakeDirs(FLAGS.checkpoint_dir)\n",
    "\n",
    "\n",
    "def print_flags():\n",
    "    \"\"\"\n",
    "    Prints all entries in FLAGS variable.\n",
    "    \"\"\"\n",
    "    for key, value in vars(FLAGS).items():\n",
    "        print(key + ' : ' + str(value))\n",
    "\n",
    "\n",
    "def str2bool(s):\n",
    "    if s == \"True\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_experiment_infos():\n",
    "    l = [\n",
    "        \"learning_rate: \" + str(FLAGS.learning_rate),\n",
    "        \"max_epochs: \" + str(FLAGS.max_epochs),\n",
    "        \"batch_size: \" + str(FLAGS.batch_size),\n",
    "        \"pretraining: \" + str(FLAGS.pretraining),\n",
    "        \"xvalidation_folds: \" + str(FLAGS.xvalidation_folds),\n",
    "        \"normalization: \" + str(FLAGS.normalization),\n",
    "        \"batch_normalization: \" + str(FLAGS.batch_normalization),\n",
    "        \"sigmas: \" + str(FLAGS.sigmas),\n",
    "        \"kernels: \" + str(FLAGS.kernels),\n",
    "        \"maps: \" + str(FLAGS.maps),\n",
    "        \"bases: \" + str(FLAGS.bases),\n",
    "        \"bases3d: \" + str(FLAGS.bases3d),\n",
    "        \"print_freq: \" + str(FLAGS.print_freq),\n",
    "        \"eval_freq: \" + str(FLAGS.eval_freq),\n",
    "        \"log_dir: \" + str(FLAGS.log_dir),\n",
    "        \"trainingpath: \" + str(FLAGS.trainingpath),\n",
    "        \"testpath: \" + str(FLAGS.testpath),\n",
    "    ]\n",
    "    return tf.convert_to_tensor(l)\n",
    "\n",
    "def get_kernels(i):\n",
    "    kernel = tf.get_default_graph().get_tensor_by_name(\"tower_0/ConvLayer%d/weights_0:0\" % i)\n",
    "    alphas = tf.get_default_graph().get_tensor_by_name(\"ConvLayer%d/alphas:0\" % i)\n",
    "#    print(kernel.get_shape())\n",
    "    \n",
    "    kernel_avg = tf.reduce_mean(kernel, axis=2)\n",
    "#    print(kernel_avg.get_shape())\n",
    "    x_min = tf.reduce_min(kernel_avg)\n",
    "    x_max = tf.reduce_max(kernel_avg)\n",
    "    kernel_0_to_1 = (kernel_avg - x_min) / (x_max - x_min)\n",
    "\n",
    "    # to tf.image_summary format [batch_size, height, width, channels]\n",
    "    kernel_transposed = tf.transpose(kernel_avg, [2, 0, 1])\n",
    "    # print(kernel_transposed.get_shape())\n",
    "\n",
    "    return alphas, kernel_transposed\n",
    "\n",
    "def show_kernels(kernels):\n",
    "    f, axarr = plt.subplots(8, 8)\n",
    "    f.set_figheight(12)\n",
    "    f.set_figwidth(12)\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            axarr[j, i].imshow(kernels[i*3+j], cmap='gray')\n",
    "            axarr[j, i].set_axis_off()\n",
    "\n",
    "def exp_GB(logits, alpha):\n",
    "    return tf.multiply(tf.exp(logits), alpha)\n",
    "\n",
    "def pow3_GB(logits, alpha, beta):\n",
    "    return tf.add(tf.multiply(tf.pow(logits, 3), alpha), beta)\n",
    "\n",
    "def tower_accuracy(logits, labels, scope):\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(softmax, 1), tf.argmax(labels, 1))\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#        tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def tower_accuracy_exp(logits, labels, scope):\n",
    "#    softmax = tf.nn.softmax(logits)\n",
    "#    signal = exp_GB(logits, FLAGS.alpha)\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#        tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "    return accuracy, correct_prediction, logits\n",
    "\n",
    "\n",
    "def tower_loss(logits, labels, scope):\n",
    "    with tf.name_scope('Cross_Entropy_Loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    with tf.name_scope('Total_Loss'):\n",
    "        total_loss = tf.add_n(tf.get_collection('losses', scope), name='total_loss')\n",
    "#        tf.summary.scalar('Total_loss', total_loss)\n",
    "    return total_loss\n",
    "\n",
    "def tower_loss_dense(logits, labels, scope):\n",
    "    with tf.name_scope('Cross_Entropy_Loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    with tf.name_scope('L2_Loss'):\n",
    "        l2_loss = tf.add_n(\n",
    "            [tf.nn.l2_loss(var) for var in tf.trainable_variables()], name='l2_loss')\n",
    "        tf.add_to_collection('losses', l2_loss * FLAGS.weight_decay)\n",
    "\n",
    "    with tf.name_scope('Total_Loss'):\n",
    "        total_loss = tf.add_n(tf.get_collection('losses', scope), name='total_loss')\n",
    " \n",
    "    return total_loss\n",
    "\n",
    "def tower_loss_exp(logits, labels, scope):\n",
    "    with tf.name_scope('Cross_Entropy_Loss'):\n",
    "#        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "#        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        signal = exp_GB(logits, FLAGS.alpha)\n",
    "        sqdiff = tf.squared_difference(signal, labels)\n",
    "        mean_rms = tf.reduce_mean(sqdiff)\n",
    "        tf.add_to_collection('losses', mean_rms)\n",
    "    with tf.name_scope('Total_Loss'):\n",
    "        total_loss = tf.add_n(tf.get_collection('losses', scope), name='total_loss')\n",
    "#        tf.summary.scalar('Total_loss', total_loss)\n",
    "    return total_loss, sqdiff\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, v in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "\n",
    "    return average_grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cifar():\n",
    "    # Set the random seeds for reproducibility. DO NOT CHANGE.\n",
    "    tf.set_random_seed(42)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            \n",
    "            global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "            # ====== LOAD DATASET ======\n",
    "            print('Loading Dataset...')\n",
    "#            cifar10_dataset = cifar10_utils.get_cifar10('/home/nicolab/Downloads/cifar-10-batches-py')\n",
    "            cifar10_dataset = cifar10_utils.get_cifar10('D:\\Adam Hilbert\\CT_Classification\\code\\Training\\cifar10\\cifar-10-batches-py')\n",
    "            print('Loading Dataset...done.')\n",
    "\n",
    "            # ====== DEFINE SPACEHOLDERS ======\n",
    "            with tf.name_scope('input'):\n",
    "                image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 32, 32, 3], name='x-input')\n",
    "                label_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 10], name='y-input')\n",
    "                is_training = tf.placeholder(tf.bool, name='is-training')\n",
    "                learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "\n",
    "            # ====== DEFINE FEED_DICTIONARY ======\n",
    "            def feed_dict(flag):\n",
    "                xs = []\n",
    "                ys = []\n",
    "                if flag == 0:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = cifar10_dataset.train.next_batch(FLAGS.batch_size)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                elif flag == 1:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = cifar10_dataset.test.next_batch(FLAGS.batch_size)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                return {image_batch: xs, label_batch: ys, is_training: flag == 0}\n",
    "\n",
    "\n",
    "            # ====== MODEL DEFINITION ======\n",
    "            print('Defining model...')\n",
    "\n",
    "            sigmas = [float(x) for x in FLAGS.sigmas.split(',')]\n",
    "            kernels = [int(x) for x in FLAGS.kernels.split(',')]\n",
    "            maps = [int(x) for x in FLAGS.maps.split(',')]\n",
    "            bases = [int(x) for x in FLAGS.bases.split(',')]\n",
    "            strides = [int(x) for x in FLAGS.strides.split(',')]\n",
    "\n",
    "#            model = RFNN(\n",
    "#                n_classes=10,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                sigmas=sigmas,\n",
    "#                bases=bases,\n",
    "#                bases_3d=FLAGS.bases3d,\n",
    "#                is_training=is_training,\n",
    "#                batchnorm=FLAGS.batch_normalization\n",
    "#            )\n",
    "#            model = CTNET(\n",
    "#                n_classes=10,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                strides=strides,\n",
    "#                pretraining=False,\n",
    "#                is_training = is_training, \n",
    "#                conv3d=False,\n",
    "#                bnorm=FLAGS.batch_normalization)\n",
    "\n",
    "            model = DenseNet(\n",
    "                growth_rate=FLAGS.growth_rate,\n",
    "                depth=FLAGS.depth,\n",
    "                total_blocks=FLAGS.total_blocks,\n",
    "                keep_prob=FLAGS.keep_prob,\n",
    "                model_type=FLAGS.model_type,\n",
    "                is_training=is_training,\n",
    "                init_kernel = FLAGS.init_kernel,\n",
    "                comp_kernel = FLAGS.comp_kernel,\n",
    "                reduction=FLAGS.reduction,\n",
    "                bc_mode=FLAGS.bc_mode,\n",
    "                n_classes=10\n",
    "            )\n",
    "\n",
    "            print('Defining model...done.')\n",
    "\n",
    "            # ====== DEFINE LOSS, ACCURACY TENSORS ======\n",
    "            print('Defining necessary OPs...')\n",
    "\n",
    "#            opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "            opt = tf.train.MomentumOptimizer(\n",
    "                FLAGS.learning_rate, FLAGS.nesterov_momentum, use_nesterov=True)\n",
    "\n",
    "\n",
    "            # === DEFINE QUEUE OPS ===\n",
    "            batch_queue = tf.FIFOQueue(\n",
    "                    capacity=NUM_GPUS,\n",
    "                    dtypes=[tf.float32, tf.float32],\n",
    "                    shapes=[ (FLAGS.batch_size,32,32,3), (FLAGS.batch_size,10) ]\n",
    "                )\n",
    "            batch_enqueue = batch_queue.enqueue_many([image_batch, label_batch])\t\t\n",
    "            close_queue = batch_queue.close()\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        tower_losses = []\n",
    "        tower_accuracies = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in xrange(NUM_GPUS):\n",
    "\n",
    "                x, y = batch_queue.dequeue()\n",
    "\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                        # ====== INFERENCE ======\n",
    "                        if FLAGS.pretraining:\n",
    "                            print('Pre-training model...')\n",
    "\n",
    "                            network_architecture = \\\n",
    "                                {\n",
    "                                    'Conv_kernels': kernels,\n",
    "                                    'Conv_maps': maps\n",
    "                                }\n",
    "                            ae = Autoencoder(network_architecture)\n",
    "\n",
    "                            assign_ops, net = ae.load_weights(x, FLAGS.pretrained_weights_path, FLAGS.pretrained_biases_path, is_training)\n",
    "\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(net)\n",
    "\n",
    "                            print('Pre-training model...done.')\n",
    "                        else:\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(x)\n",
    "\n",
    "                        loss = tower_loss_dense(logits, y, scope)\n",
    "                        accuracy = tower_accuracy(logits, y, scope)\n",
    "#                        exp_signal = exp_GB(signal, FLAGS.alpha)\n",
    " \n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                        # Retain the summaries from the final tower.\n",
    "                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                        # Calculate the gradients for the batch of data on this tower.\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "\n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_accuracies.append(accuracy)\n",
    "        with tf.device('/cpu:0'):\n",
    "\n",
    "            # Calculate the mean of each gradient - synchronization point across towers.\n",
    "            avg_grads = average_gradients(tower_grads)\n",
    "            avg_loss = tf.reduce_mean(tower_losses, 0)\n",
    "            avg_accuracy = tf.reduce_mean(tower_accuracies, 0)\n",
    "\n",
    "            print('Defining necessary OPs...done.')\n",
    "\n",
    "            # ====== ADD SUMMARIES ======\n",
    "\n",
    "            # Gradients\n",
    "            for grad, var in avg_grads:\n",
    "                if grad is not None:\n",
    "                    summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\t\n",
    "\n",
    "            # Trainable variables\n",
    "            for var in tf.trainable_variables():\n",
    "                summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "\n",
    "            # Print initial kernels\n",
    "            # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "            # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "            # np.save('./Kernels/kernel_0.npy', kernels_array)\n",
    "            # np.save('./Kernels/alphas_0.npy', alphas)\n",
    "\n",
    "            # ====== UPDATE VARIABLES ======\n",
    "\n",
    "            print('Defining update OPs...')\n",
    "\n",
    "            # Apply the gradients to adjust the shared variables.\n",
    "            train_op = opt.apply_gradients(avg_grads, global_step=global_step)\n",
    "\n",
    "            # Track the moving averages of all trainable variables.\n",
    "#            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "#                        0.9999, global_step)\n",
    "#            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "            # Group all updates into a single train op.\n",
    "#            train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "            print('Defining update OPs...done.')\n",
    "\n",
    "            # ====== SAVING OPS ======\n",
    "\n",
    "            # Create a saver.\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            # Build the summary operation from the last tower summaries.\n",
    "            summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "            # ====== DEFINE SESSION AND OPTIMIZE ======\n",
    "            config = tf.ConfigProto(allow_soft_placement=True)\n",
    "            config.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "                \n",
    "            print('Training model...')\n",
    "            for f in range(1):\n",
    "                try:\n",
    "                    training_steps = int(cifar10_dataset.train.num_examples / (NUM_GPUS*FLAGS.batch_size))\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                    lr = FLAGS.learning_rate\n",
    "                    \n",
    "#                    print([n.name for n in tf.get_default_graph().as_graph_def().node\n",
    "#                               if 'ConvLayer1' in n.name and 'alphas' in n.name])\n",
    "                    \n",
    "                    # Show initial kernels from 1st layer\n",
    "#                    alphas_t, kernels_t = get_kernels(1)\n",
    "#                    alphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "#                    show_kernels(kernels)\n",
    "                    \n",
    "                    # Create a coordinator, launch the queue runner threads.\n",
    "                    coord = tf.train.Coordinator()\n",
    "                    tf.train.start_queue_runners(sess, coord=coord)\n",
    "\n",
    "                    # Assign ops if pre-training\n",
    "                    if FLAGS.pretraining:\n",
    "                        for assign_op in assign_ops:\n",
    "                            sess.run(assign_op)\n",
    "\n",
    "                    # Init writers\n",
    "                    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train/' + str(f), sess.graph)\n",
    "                    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test/'  + str(f))\n",
    "                    \n",
    "                    max_acc = 0\n",
    "                    for i in range(int(FLAGS.max_epochs * training_steps)):\n",
    "\n",
    "                        if coord.should_stop():\n",
    "                            break\n",
    "                        \n",
    "                        # ----- Reduce learning rate ------\n",
    "                        if i == FLAGS.reduce_lr_epoch_1 * training_steps:\n",
    "                            lr = lr / 10\n",
    "                        if i == FLAGS.reduce_lr_epoch_2 * training_steps:\n",
    "                            lr = lr / 10\n",
    "\n",
    "                        # ------------ TRAIN -------------\n",
    "                        if i % (FLAGS.print_freq * training_steps) == 0:\n",
    "                            # ------------ PRINT -------------\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, summaries, loss_value, acc_value = \\\n",
    "                                    sess.run([train_op, summary_op, avg_loss, avg_accuracy],\n",
    "                                             feed_dict={is_training: True, learning_rate: lr})\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=acc_value)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=loss_value)\n",
    "                            summary.value.add(tag=\"Learning_rate\", simple_value=lr)\n",
    "\n",
    "                            train_writer.add_summary(summaries, i)\n",
    "                            train_writer.add_summary(summary, i)\n",
    "                        else:\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, loss_value = sess.run([train_op, avg_loss], \n",
    "                                                feed_dict={is_training: True, learning_rate: lr})\n",
    "\n",
    "                        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "\n",
    "                        if i % (FLAGS.eval_freq * training_steps) == 0 or i == int(FLAGS.max_epochs * training_steps):\n",
    "                            # ------------ VALIDATON -------------\n",
    "                            validation_steps = int(cifar10_dataset.test.num_examples / (FLAGS.batch_size*NUM_GPUS))\n",
    "                            tot_acc = 0.0\n",
    "                            tot_loss = 0.0\t\t\t\t\n",
    "                            for step in range(validation_steps):\n",
    "                                sess.run(batch_enqueue, feed_dict=feed_dict(1))\n",
    "                                acc_s, loss_s = sess.run([avg_accuracy, avg_loss], feed_dict={is_training: False})\n",
    "                                tot_acc += (acc_s / validation_steps)\n",
    "                                tot_loss += (loss_s / validation_steps)\n",
    "        \n",
    "#                                print(sm)\n",
    "#                                print(y_)\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=tot_acc)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=tot_loss)\n",
    "\n",
    "                            test_writer.add_summary(summary, i)\n",
    "\n",
    "    #\t\t\t\t\t\tif tot_acc > max_acc:\n",
    "    #\t\t\t\t\t\t\tmax_acc = tot_acc\n",
    "    #\t\t\t\t\t\tprint('Validation accuracy at step %s: %s' % (i, tot_acc))\n",
    "                            print('Validation loss at step %s: %s' % (i, tot_loss))\n",
    "\n",
    "\n",
    "                        # if i % FLAGS.checkpoint_freq == 0: # or i == FLAGS.max_steps:\n",
    "                        # checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n",
    "                        # saver.save(sess, checkpoint_path, global_step=i)\n",
    "\n",
    "                    train_writer.close()\n",
    "                    test_writer.close()\n",
    "                    print('Max validation accuracy in fold %s: %s' % (f,max_acc))\n",
    "                    \n",
    "                    # Show final kernels from 1st layer\n",
    "#                    alphas_t, kernels_t = get_kernels(1)\n",
    "#                    alphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "#                    show_kernels(kernels)\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Report exceptions to the coordinator.\n",
    "                    coord.request_stop(e)\n",
    "                finally:\n",
    "                    # Terminate threads\n",
    "                    coord.request_stop()\n",
    "                    coord.join()\n",
    "            sess.run(close_queue)\n",
    "\n",
    "\n",
    "    # Print final kernels\n",
    "    # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "    # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "    # np.save('./Kernels/kernel_final.npy', kernels_array)\n",
    "    # np.save('./Kernels/alphas_final.npy', alphas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ctnet():\n",
    "    # Set the random seeds for reproducibility. DO NOT CHANGE.\n",
    "    tf.set_random_seed(42)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/cpu:0'):\n",
    "            global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "\n",
    "            # ====== LOAD DATASET ======\n",
    "            print('Loading Dataset...')\n",
    "            with open(FLAGS.trainingpath, 'rb') as handle:\n",
    "                training_points = pickle.load(handle)\n",
    "            with open(FLAGS.testpath, 'rb') as handle:\n",
    "                test_points = pickle.load(handle)\n",
    "\n",
    "            dataset = utils.DataSet(training_points, test_points,\n",
    "                    cross_validation_folds=FLAGS.xvalidation_folds,\n",
    "                    normalize = FLAGS.normalization)\n",
    "            print('Loading Dataset...done.')\n",
    "\n",
    "            # ====== DEFINE SPACEHOLDERS ======\n",
    "            with tf.name_scope('input'):\n",
    "                if FLAGS.bases3d:\n",
    "                    image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 512, 512, 30, 1], name='x-input')\n",
    "                else:\n",
    "                    image_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 512, 512, 3], name='x-input')\n",
    "                label_batch = tf.placeholder(tf.float32, [NUM_GPUS, FLAGS.batch_size, 2], name='y-input')\n",
    "                is_training = tf.placeholder(tf.bool, name='is-training')\n",
    "                learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "\n",
    "            # ====== DEFINE FEED_DICTIONARY ======\n",
    "            def feed_dict(flag):\n",
    "                xs = []\n",
    "                ys = []\n",
    "                if flag == 0:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = dataset.Training.next_batch(FLAGS.batch_size, bases3d=FLAGS.bases3d)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                elif flag == 1:\n",
    "                    for i in np.arange(NUM_GPUS):\n",
    "                        xi, yi = dataset.Validation.next_batch(FLAGS.batch_size, bases3d=FLAGS.bases3d)\n",
    "                        xs.append(xi)\n",
    "                        ys.append(yi)\n",
    "                return {image_batch: xs, label_batch: ys, is_training: flag == 0}\n",
    "\n",
    "\n",
    "            # ====== MODEL DEFINITION ======\n",
    "            print('Defining model...')\n",
    "\n",
    "            sigmas = [float(x) for x in FLAGS.sigmas.split(',')]\n",
    "            kernels = [int(x) for x in FLAGS.kernels.split(',')]\n",
    "            maps = [int(x) for x in FLAGS.maps.split(',')]\n",
    "            bases = [int(x) for x in FLAGS.bases.split(',')]\n",
    "            strides = [int(x) for x in FLAGS.strides.split(',')]\n",
    "\n",
    "#            model = RFNN(\n",
    "#                n_classes=2,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                sigmas=sigmas,\n",
    "#                bases=bases,\n",
    "#                bases_3d=FLAGS.bases3d,\n",
    "#                is_training=is_training,\n",
    "#                batchnorm=FLAGS.batch_normalization\n",
    "#            )\n",
    "\n",
    "#            model = CTNET(\n",
    "#                n_classes=2,\n",
    "#                kernels=kernels,\n",
    "#                maps=maps,\n",
    "#                strides=strides,\n",
    "#                pretraining=False,\n",
    "#                is_training = is_training, \n",
    "#                conv3d=FLAGS.bases3d,\n",
    "#                bnorm=FLAGS.batch_normalization)\n",
    "\n",
    "            model = DenseNet(\n",
    "                growth_rate=FLAGS.growth_rate,\n",
    "                depth=FLAGS.depth,\n",
    "                total_blocks=FLAGS.total_blocks,\n",
    "                keep_prob=FLAGS.keep_prob,\n",
    "                model_type=FLAGS.model_type,\n",
    "                is_training=is_training,\n",
    "                init_kernel = FLAGS.init_kernel,\n",
    "                comp_kernel = FLAGS.comp_kernel,\n",
    "                reduction=FLAGS.reduction,\n",
    "                bc_mode=FLAGS.bc_mode,\n",
    "                n_classes=2\n",
    "            )\n",
    "    \n",
    "            print('Defining model...done.')\n",
    "\n",
    "            # ====== DEFINE LOSS, ACCURACY TENSORS ======\n",
    "            print('Defining necessary OPs...')\n",
    "\n",
    "#            opt = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "            opt = tf.train.MomentumOptimizer(\n",
    "                FLAGS.learning_rate, FLAGS.nesterov_momentum, use_nesterov=True)\n",
    "\n",
    "\n",
    "            # === DEFINE QUEUE OPS ===\n",
    "            batch_queue = tf.FIFOQueue(\n",
    "                capacity=NUM_GPUS,\n",
    "                dtypes=[tf.float32, tf.float32],\n",
    "                shapes=[ (FLAGS.batch_size,512,512,30,1) if FLAGS.bases3d else (FLAGS.batch_size,512,512,3),\n",
    "                             (FLAGS.batch_size,2) ]\n",
    "            )\n",
    "            batch_enqueue = batch_queue.enqueue_many([image_batch, label_batch])\t\t\n",
    "            close_queue = batch_queue.close()\n",
    "\n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        tower_losses = []\n",
    "        tower_accuracies = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in xrange(NUM_GPUS):\n",
    "\n",
    "                x, y = batch_queue.dequeue()\n",
    "\n",
    "                with tf.device('/gpu:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\n",
    "                        # ====== INFERENCE ======\n",
    "                        if FLAGS.pretraining:\n",
    "                            print('Pre-training model...')\n",
    "\n",
    "                            network_architecture = \\\n",
    "                            {\n",
    "                            'Conv_kernels': kernels,\n",
    "                            'Conv_maps': maps\n",
    "                            }\n",
    "                            ae = Autoencoder(network_architecture)\n",
    "\n",
    "                            assign_ops, net = ae.load_weights(x, FLAGS.pretrained_weights_path, FLAGS.pretrained_biases_path, is_training)\n",
    "\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(net)\n",
    "\n",
    "                            print('Pre-training model...done.')\n",
    "                        else:\n",
    "                            # Calculate predictions\n",
    "                            logits = model.inference(x)\n",
    "\n",
    "                        loss = tower_loss_dense(logits, y, scope)\n",
    "                        accuracy = tower_accuracy(logits, y, scope)\n",
    "\n",
    "                        # Reuse variables for the next tower.\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                        # Retain the summaries from the final tower.\n",
    "                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                        # Calculate the gradients for the batch of data on this tower.\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "\n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "                        tower_losses.append(loss)\n",
    "                        tower_accuracies.append(accuracy)\n",
    "        with tf.device('/cpu:0'):\n",
    "\n",
    "            # Calculate the mean of each gradient - synchronization point across towers.\n",
    "            avg_grads = average_gradients(tower_grads)\n",
    "            avg_loss = tf.reduce_mean(tower_losses, 0)\n",
    "            avg_accuracy = tf.reduce_mean(tower_accuracies, 0)\n",
    "\n",
    "            print('Defining necessary OPs...done.')\n",
    "\n",
    "            # ====== ADD SUMMARIES ======\n",
    "\n",
    "            # Gradients\n",
    "            for grad, var in avg_grads:\n",
    "                if grad is not None:\n",
    "                        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))\t\n",
    "\n",
    "            # Trainable variables\n",
    "            for var in tf.trainable_variables():\n",
    "                summaries.append(tf.summary.histogram(var.op.name, var))\n",
    "\n",
    "\n",
    "            # Print initial kernels\n",
    "            # alphas_tensor, kernels_tensor = get_kernels()\t\n",
    "            # alphas, kernels_array = sess.run([alphas_tensor, kernels_tensor])\t\t\n",
    "            # np.save('./Kernels/kernel_0.npy', kernels_array)\n",
    "            # np.save('./Kernels/alphas_0.npy', alphas)\n",
    "\n",
    "            # ====== UPDATE VARIABLES ======\n",
    "\n",
    "            print('Defining update OPs...')\n",
    "\n",
    "            # Apply the gradients to adjust the shared variables.\n",
    "            train_op = opt.apply_gradients(avg_grads, global_step=global_step)\n",
    "\n",
    "            # Track the moving averages of all trainable variables.\n",
    "#            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "#                0.9999, global_step)\n",
    "#            variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "            # Group all updates into a single train op.\n",
    "#            train_op = tf.group(apply_gradient_op, variables_averages_op)\n",
    "\n",
    "            print('Defining update OPs...done.')\n",
    "\n",
    "            # ====== SAVING OPS ======\n",
    "\n",
    "            # Create a saver.\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            # Build the summary operation from the last tower summaries.\n",
    "            summary_op = tf.summary.merge(summaries)\n",
    "\n",
    "            # ====== DEFINE SESSION AND OPTIMIZE ======\n",
    "            config = tf.ConfigProto(allow_soft_placement=True)\n",
    "            config.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "\n",
    "            print('Training model...')\n",
    "\n",
    "            for f in range(1):\n",
    "\n",
    "                # Create a coordinator, launch the queue runner threads.\n",
    "                coord = tf.train.Coordinator()\n",
    "                tf.train.start_queue_runners(sess, coord=coord)\n",
    "\n",
    "                lr = FLAGS.learning_rate\n",
    "                \n",
    "                try:\n",
    "                    training_steps = int(np.ceil(2*dataset.Training.num_examples / (NUM_GPUS*FLAGS.batch_size)))\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            #                    \tprint([n.name for n in tf.get_default_graph().as_graph_def().node\n",
    "            #                               if 'ConvLayer1' in n.name and 'alphas' in n.name])\n",
    "\n",
    "                    # Show initial kernels from 1st layer\n",
    "    #\t\t\t\talphas_t, kernels_t = get_kernels(1)\n",
    "    #\t\t\t\talphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "    #\t\t\t\tshow_kernels(kernels)\n",
    "\n",
    "                    # Assign ops if pre-training\n",
    "                    if FLAGS.pretraining:\n",
    "                        for assign_op in assign_ops:\n",
    "                            sess.run(assign_op)\n",
    "\n",
    "                    # Init writers\n",
    "                    train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train/' + str(f), sess.graph)\n",
    "                    test_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test/'  + str(f))\n",
    "\n",
    "                    max_acc = 0\n",
    "                    for i in range(int(FLAGS.max_epochs * training_steps)):\n",
    "\n",
    "                        if coord.should_stop():\n",
    "                            break\n",
    "                            \n",
    "                        # ----- Reduce learning rate ------\n",
    "                        if i == FLAGS.reduce_lr_epoch_1 * training_steps:\n",
    "                            lr = lr / 10\n",
    "                        if i == FLAGS.reduce_lr_epoch_2 * training_steps:\n",
    "                            lr = lr / 10\n",
    "\n",
    "                        # ------------ TRAIN -------------                        \n",
    "                        if i % (FLAGS.print_freq * training_steps) == 0:\n",
    "                            # ------------ PRINT -------------\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, summaries, loss_value, acc_value = sess.run([train_op, summary_op, avg_loss, avg_accuracy], feed_dict={is_training: True})\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=acc_value)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=loss_value)\n",
    "                            summary.value.add(tag=\"Learning_rate\", simple_value=lr)\n",
    "                            \n",
    "                            train_writer.add_summary(summaries, i)\n",
    "                            train_writer.add_summary(summary, i)\n",
    "                        else:\n",
    "                            sess.run(batch_enqueue, feed_dict=feed_dict(0))\n",
    "                            _, loss_value = sess.run([train_op, avg_loss], feed_dict={is_training: True})\n",
    "\n",
    "                        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "                        if i % (FLAGS.eval_freq * training_steps) == 0 or i == int(FLAGS.max_epochs * training_steps):\n",
    "                            # ------------ VALIDATON -------------\n",
    "                            validation_steps = int(np.ceil(2*dataset.Validation.num_examples / (FLAGS.batch_size*NUM_GPUS)))\n",
    "                            tot_acc = 0.0\n",
    "                            tot_loss = 0.0\t\t\t\t\n",
    "                            for step in range(validation_steps):\n",
    "                                sess.run(batch_enqueue, feed_dict=feed_dict(1))\n",
    "                                acc_s, loss_s = sess.run([avg_accuracy, avg_loss], feed_dict={is_training: False})\n",
    "                                # print(pred)\n",
    "                                tot_acc += (acc_s / validation_steps)\n",
    "                                tot_loss += (loss_s / validation_steps)\n",
    "\n",
    "                            summary = tf.Summary()\n",
    "                            summary.value.add(tag=\"Accuracy\", simple_value=tot_acc)\n",
    "                            summary.value.add(tag=\"Loss\", simple_value=tot_loss)\n",
    "\n",
    "                            test_writer.add_summary(summary, i)\n",
    "\n",
    "                            if tot_acc > max_acc:\n",
    "                                max_acc = tot_acc\n",
    "                            print('Validation loss at step %s: %s' % (i, tot_loss))\n",
    "\n",
    "\n",
    "                        # if i % FLAGS.checkpoint_freq == 0: # or i == FLAGS.max_steps:\n",
    "                        # checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n",
    "                        # saver.save(sess, checkpoint_path, global_step=i)\n",
    "\n",
    "                    train_writer.close()\n",
    "                    test_writer.close()\n",
    "                    print('Max validation accuracy in fold %s: %s' % (f,max_acc))\n",
    "\n",
    "                    # Show final kernels from 1st layer\n",
    "    #\t\t\t\talphas_t, kernels_t = get_kernels(1)\n",
    "    #\t\t\t\talphas, kernels = sess.run([alphas_t, kernels_t])\n",
    "    #\t\t\t\tshow_kernels(kernels)\n",
    "\n",
    "    #\t\t\tdataset.next_fold()\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Report exceptions to the coordinator.\n",
    "                    coord.request_stop(e)\n",
    "                finally:\n",
    "                    # Terminate threads\n",
    "                    coord.request_stop()\n",
    "                    coord.join()\n",
    "            sess.run(close_queue)\n",
    "            # Save final model\n",
    "            checkpoint_path = os.path.join(FLAGS.checkpoint_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "FLAGS = FLAGS()\n",
    "\n",
    "# Training\n",
    "FLAGS.weight_decay = 1e-4\n",
    "FLAGS.nesterov_momentum = 0.9\n",
    "# FLAGS.learning_rate = 0.001\n",
    "FLAGS.max_epochs = 100\n",
    "FLAGS.batch_size = 64\n",
    "FLAGS.pretraining = False\n",
    "FLAGS.xvalidation_folds = 10\n",
    "# FLAGS.reduce_lr_epoch_1 = 50\n",
    "# FLAGS.reduce_lr_epoch_2 = 50\n",
    "\n",
    "# Exp-Gradient\n",
    "FLAGS.alpha = 0.1\n",
    "\n",
    "# Data\n",
    "#FLAGS.normalization = False\n",
    "#FLAGS.trainingpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_SKULLSTRIPPED_RESAMPLED/training_points.npy'\n",
    "#FLAGS.testpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_SKULLSTRIPPED_RESAMPLED/test_points.npy'\n",
    "#FLAGS.trainingpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_CTA/training_points.npy'\n",
    "#FLAGS.testpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_CTA/test_points.npy'\n",
    "\n",
    "# General CNN\n",
    "FLAGS.batch_normalization = True\n",
    "FLAGS.kernels = '11,5,5'\n",
    "FLAGS.strides = '3,2,3'\n",
    "FLAGS.maps = '32,32,2'\n",
    "FLAGS.bases3d = False\n",
    "\n",
    "# DenseNet\n",
    "# FLAGS.growth_rate = 8\n",
    "# FLAGS.depth = 40\n",
    "FLAGS.total_blocks = 3\n",
    "# FLAGS.keep_prob = 0.3\n",
    "FLAGS.reduction = 1.0\n",
    "FLAGS.model_type = 'DenseNet'\n",
    "FLAGS.bc_mode = False\n",
    "# FLAGS.init_kernel = 11\n",
    "# FLAGS.comp_kernel = 5\n",
    "\n",
    "# RFNN\n",
    "FLAGS.sigmas = '1.5'\n",
    "FLAGS.bases = '10,10,10'\n",
    "\n",
    "# Logging\n",
    "FLAGS.print_freq = 10\n",
    "FLAGS.eval_freq = 10\n",
    "FLAGS.checkpoint_freq = 0\n",
    "# FLAGS.log_dir = '/home/nicolab/DATA/logs/NCCT_CTA/DenseNet/' \\\n",
    "#     + str(FLAGS.init_kernel) + 'x' + str(FLAGS.comp_kernel) + '_' \\\n",
    "#     + str(FLAGS.learning_rate) + '_' \\\n",
    "#     + str(FLAGS.growth_rate) + '_' \\\n",
    "#     + str(FLAGS.depth) + '_' \\\n",
    "#     + str(FLAGS.total_blocks) + '_' \\\n",
    "#     + str(FLAGS.keep_prob) + '_' \\\n",
    "#     + str(FLAGS.reduction)\n",
    "\n",
    "#    + str(FLAGS.reduce_lr_epoch_1) + '_' \\\n",
    "    \n",
    "\n",
    "configs = [\n",
    "     [11, 5, 0.1, 12, 0.8, 40, 50, 75],\n",
    "#     [11, 5, 0.0005, 4, 0.6, 50, 50],\n",
    "#     [11, 5, 0.0005, 8, 0.6, 4, 8],\n",
    "#    [17, 5, 0.0005, 4, 0.4, 10, 50],\n",
    "#    [21, 5, 0.0005, 4, 0.4, 50, 50],\n",
    "#    [21, 11, 0.0005, 8, 0.6, 5, 10],\n",
    "#    [21, 11, 0.0005, 4, 0.4, 50, 50]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Loading Dataset...done.\n",
      "Defining model...\n",
      "Build DenseNet model with 3 blocks, 12 composite layers each.\n",
      "Reduction at transition layers: 1.0\n",
      "Defining model...done.\n",
      "Defining necessary OPs...\n",
      "Defining necessary OPs...done.\n",
      "Defining update OPs...\n",
      "Defining update OPs...done.\n",
      "Training model...\n",
      "Validation loss at step 0: 4.29742314571\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    FLAGS.init_kernel = config[0]\n",
    "    FLAGS.comp_kernel = config[1]\n",
    "    FLAGS.learning_rate = config[2]\n",
    "    FLAGS.growth_rate = config[3]\n",
    "    FLAGS.keep_prob = config[4]\n",
    "    FLAGS.depth = config[5]\n",
    "    FLAGS.reduce_lr_epoch_1 = config[6]\n",
    "    FLAGS.reduce_lr_epoch_2 = config[7]\n",
    "\n",
    "#    FLAGS.log_dir = '/home/nicolab/DATA/logs/NCCT_CTA/DenseNet/bests/' \n",
    "    FLAGS.log_dir = 'D:\\Adam Hilbert\\CT_Classification\\code\\Training\\logs\\\\' \\\n",
    "        + str(FLAGS.init_kernel) + 'x' + str(FLAGS.comp_kernel) + '_' \\\n",
    "        + str(FLAGS.learning_rate) + '_' \\\n",
    "        + str(FLAGS.growth_rate) + '_' \\\n",
    "        + str(FLAGS.depth) + '_' \\\n",
    "        + str(FLAGS.total_blocks) + '_' \\\n",
    "        + str(FLAGS.keep_prob) + '_' \\\n",
    "        + str(FLAGS.reduction)\n",
    "#    FLAGS.checkpoint_dir = '/home/nicolab/DATA/checkpoints/NCCT_CTA/DenseNet/' \\\n",
    "    FLAGS.checkpoint_dir = 'D:\\Adam Hilbert\\CT_Classification\\code\\Training\\checkpoints\\\\' \\\n",
    "        + str(FLAGS.init_kernel) + 'x' + str(FLAGS.comp_kernel) + '_' \\\n",
    "        + str(FLAGS.learning_rate) + '_' \\\n",
    "        + str(FLAGS.growth_rate) + '_' \\\n",
    "        + str(FLAGS.depth) + '_' \\\n",
    "        + str(FLAGS.total_blocks) + '_' \\\n",
    "        + str(FLAGS.keep_prob) + '_' \\\n",
    "        + str(FLAGS.reduction)\n",
    "\n",
    "    initialize_folders()\n",
    "#    train_ctnet()\n",
    "    train_cifar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
