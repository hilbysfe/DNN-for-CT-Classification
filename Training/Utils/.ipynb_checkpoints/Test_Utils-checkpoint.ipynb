{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import utils\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_RESAMPLED/training_points.npy'\n",
    "testpath = '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_RESAMPLED/test_points.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_RESAMPLED/training_points.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4b5ea618cc1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtraining_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nicolab/DATA/data_binaries/SUPERVISED/NCCT_THICK_RESAMPLED/training_points.npy'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(trainingpath, 'rb') as handle:\n",
    "    training_points = pickle.load(handle)\n",
    "with open(testpath, 'rb') as handle:\n",
    "    test_points = pickle.load(handle)\n",
    "\n",
    "dataset = DataSet(np.array(list(training_points.keys())), np.array(list(training_points.values())),\n",
    "            np.array(list(test_points.keys())), np.array(list(test_points.values())),\n",
    "            cross_validation_folds=10,\n",
    "            normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import normalize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r'D:\\Adam Hilbert\\Data\\R0006.mha'\n",
    "out = r'D:\\Adam Hilbert\\Data\\R0006n.mha'\n",
    "\n",
    "data = sitk.GetArrayFromImage(sitk.ReadImage(path))\n",
    "norm = normalize_image(data)\n",
    "\n",
    "sitk.WriteImage(sitk.GetImageFromArray(norm), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test Datautils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import openpyxl as ox\n",
    "import operator\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "MIN_BOUND = 0.0\n",
    "MAX_BOUND = 200.0\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op\n",
    "\n",
    "\n",
    "def _activation_summary(x):\n",
    "    tensor_name = x.op.name\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "\n",
    "def _variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + var.op.name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.scalar_summary('stddev/' + var.op.name, stddev)\n",
    "        tf.scalar_summary('max/' + var.op.name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + var.op.name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "\n",
    "def online_flattened_mean(files):\n",
    "    # Calculates mean of all pixels in the dataset, considering the intensities normalized\n",
    "    mean = 0\n",
    "    for file in files:\n",
    "        data = normalize_image(sitk.GetArrayFromImage(sitk.ReadImage(file)))\n",
    "        mean += np.sum(data)\n",
    "\n",
    "    return mean/(len(files)*np.shape(data)[0]*np.shape(data)[1]*np.shape(data)[2])\n",
    "\n",
    "def normalize_image(data):\n",
    "    data = (data-MIN_BOUND) / (MAX_BOUND-MIN_BOUND)\n",
    "    data[data>1] = 1\n",
    "    data[data<0] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"\n",
    "    Convert class labels from scalars to one-hot vectors.\n",
    "    Args:\n",
    "    labels_dense: Dense labels.\n",
    "    num_classes: Number of classes.\n",
    "\n",
    "    Outputs:\n",
    "    labels_one_hot: One-hot encoding for labels.\n",
    "    \"\"\"\n",
    "\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def read_dataset(datapath, labelpath, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Function to read up images and labels.\n",
    "    Store only paths as images wouldn't fit to memory.\n",
    "\n",
    "    MRS@90 hardcoded as label attribute in label_filename -> column 'DF'\n",
    "    \"\"\"\n",
    "\n",
    "    followid_attribute = 'A2:A1489'\n",
    "    label_attribute = 'DF2:DF1489'\n",
    "\n",
    "    # --- Retrieve all patients we have images from ---\n",
    "    patients = os.listdir(datapath)\n",
    "\n",
    "    # --- Load labels from file ---\n",
    "    labels_wb = ox.load_workbook(labelpath)\n",
    "    labels_ws = labels_wb['Registrydatabase']\n",
    "\n",
    "    label_dict = {key[0].value: value[0].value \n",
    "        for i, (key, value) in enumerate(zip(labels_ws[followid_attribute], labels_ws[label_attribute]))\n",
    "                                  if labels_ws['Z'+str(i+2)].value == 3 }\n",
    "\n",
    "    # --- Filter out the ones missing from Database ---\n",
    "    patients = [ name for name in patients if name in label_dict.keys() ]\t\n",
    "\n",
    "    # --- Split regarding classes ---\n",
    "    class0_images = [os.path.join(root, name)\n",
    "                              for root, dirs, files in os.walk(datapath)\n",
    "                              for name in files if name.endswith(\".mha\")\n",
    "                                    if name.split('.')[0] in label_dict.keys() and\n",
    "                                         label_dict[name.split('.')[0]] == 0]\n",
    "    class1_images = [os.path.join(root, name)\n",
    "                              for root, dirs, files in os.walk(datapath)\n",
    "                              for name in files if name.endswith(\".mha\")\n",
    "                                    if name.split('.')[0] in label_dict.keys() and\n",
    "                                         label_dict[name.split('.')[0]] == 1]\n",
    "    \n",
    "    num_examples = len(class0_images) + len(class1_images)\n",
    "\n",
    "    # --- Schuffle both classes ---\n",
    "    perm = np.arange(len(class0_images))\n",
    "    np.random.shuffle(perm)\n",
    "    class0_images = np.array(class0_images)[perm]\n",
    "    \n",
    "    perm = np.arange(len(class1_images))\n",
    "    np.random.shuffle(perm)\n",
    "    class1_images = np.array(class1_images)[perm]\n",
    "    \n",
    "    # --- Split into balanced test-set and unbalanced training ---\n",
    "    test_size = int(num_examples*test_ratio)    \n",
    "    test_points = dict(\n",
    "        zip(np.concatenate((class0_images[:int(test_size/2)], class1_images[:int(test_size/2)])),\n",
    "            np.concatenate((np.zeros((int(test_size/2),), dtype=np.int), np.ones((int(test_size/2),), dtype=np.int)))))\n",
    "    \n",
    "    training_points = dict(\n",
    "        zip(np.concatenate((class0_images[int(test_size/2):], class1_images[int(test_size/2):])),\n",
    "            np.concatenate((np.zeros((len(class0_images)-int(test_size/2),), dtype=np.int), np.ones((len(class1_images)-int(test_size/2),), dtype=np.int)))))\n",
    "    \n",
    "    return training_points, test_points\n",
    "\n",
    "\n",
    "def split_dataset(datapath, labelpath, output_path):\n",
    "\n",
    "    training_points, test_points = read_dataset(datapath, labelpath)\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    with open(os.path.join(output_path, 'training_points.npy'), 'wb') as handle:\n",
    "        pickle.dump(training_points, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(output_path, 'test_points.npy'), 'wb') as handle:\n",
    "        pickle.dump(test_points, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return training_points, test_points\t\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, training_points, test_points, cross_validation_folds=0, normalize=False):\n",
    "        print('Init Dataset...')\n",
    "        # === TEST-SET ===\n",
    "        # --- Split and shuffle ---\n",
    "        perm = np.arange(int(len(test_points)/2))\n",
    "        np.random.shuffle(perm)\n",
    "        test_images0 = np.array([ image for image in test_points if test_points[image] == 0])[perm]\n",
    "        test_images1 = np.array([ image for image in test_points if test_points[image] == 1])[perm]\n",
    "        test_labels0 = np.array([ np.ndarray((2,), buffer=np.array([0, 1]), dtype=int) for i in range(len(test_images0)) ])\n",
    "        test_labels1 = np.array([ np.ndarray((2,), buffer=np.array([1, 0]), dtype=int) for i in range(len(test_images1)) ])\n",
    "        self._Test = SubSet(test_images0, test_images1, test_labels0, test_labels1)\n",
    "        \n",
    "        # === TRAINING-SET ===\n",
    "        # --- Split and shuffle ---\n",
    "        training_images0 = np.array([ image for image in training_points if training_points[image] == 0])\n",
    "        perm = np.arange(len(training_images0))\n",
    "        np.random.shuffle(perm)\n",
    "        training_images0 = training_images0[perm]\n",
    "\n",
    "        training_images1 = np.array([ image for image in training_points if training_points[image] == 1])\n",
    "        perm = np.arange(len(training_images1))\n",
    "        np.random.shuffle(perm)\n",
    "        training_images1 = training_images1[perm]\n",
    "\n",
    "        # --- Balance out ---\n",
    "        balanced_size = len(training_images0) if len(training_images0)<len(training_images1) else len(training_images1)\n",
    "        \n",
    "        training_images0 = training_images0[:balanced_size]\n",
    "        training_images1 = training_images1[:balanced_size]\n",
    "        training_labels0 = np.array([ np.ndarray((2,), buffer=np.array([0, 1]), dtype=int) for i in range(len(training_images0)) ])\n",
    "        training_labels1 = np.array([ np.ndarray((2,), buffer=np.array([1, 0]), dtype=int) for i in range(len(training_images1)) ])\n",
    "        \n",
    "        if cross_validation_folds == 0:\n",
    "            \n",
    "            self._Training = SubSet(training_images0, training_images1, training_labels0, training_labels1)\n",
    "            \n",
    "            if normalize:\n",
    "                self.Normalization = True\n",
    "                self._Training.Normalization = True\n",
    "                self._Test.Normalization = True\n",
    "                \n",
    "                print('Computing mean...')\n",
    "                mean = online_flattened_mean(self._Training.images)\n",
    "                print('Computing mean...done.')\n",
    "\n",
    "                self._Training.setNormalizationParameters(mean)\n",
    "                self._Test.setNormalizationParameters(mean)\n",
    "            else:\n",
    "                self.Normalization = False\n",
    "                self._Training.Normalization = False\n",
    "                self._Test.Normalization = False\n",
    "\n",
    "        else:\n",
    "            print('Creating folds...')\n",
    "            \n",
    "            # === TRAINING + VALIDATION-SET ===\n",
    "            self._current_fold = 0\n",
    "            self._point_per_fold = int(balanced_size/cross_validation_folds)\n",
    "            \n",
    "            perm = np.arange(2*self._point_per_fold)\n",
    "            np.random.shuffle(perm)\n",
    "            \n",
    "            # --- Prepare folds ---\n",
    "            self._image0_folds = []\n",
    "            self._label0_folds = []\n",
    "            self._image1_folds = []\n",
    "            self._label1_folds = []\n",
    "            offset = 0\n",
    "            for i in range(cross_validation_folds):\n",
    "                self._image0_folds.append(\n",
    "                    training_images0[offset:offset+self._point_per_fold]\n",
    "                )\n",
    "                self._image1_folds.append(\n",
    "                    training_images1[offset:offset+self._point_per_fold]\n",
    "                )\n",
    "                self._label0_folds.append(\n",
    "                    training_labels0[offset:offset+self._point_per_fold]\n",
    "                )\n",
    "                self._label1_folds.append(\n",
    "                    training_labels1[offset:offset+self._point_per_fold]\n",
    "                )\n",
    "                \n",
    "                offset += self._point_per_fold\n",
    "\n",
    "            # --- Init sets with 1st fold ---\n",
    "            training_imgset0 = np.concatenate( [ fold for i,fold in enumerate(self._image0_folds) if i != self._current_fold] )\n",
    "            training_labelset0 = np.concatenate( [ fold for i,fold in enumerate(self._label0_folds) if i != self._current_fold] )\n",
    "            validation_imgset0 = self._image0_folds[self._current_fold]\n",
    "            validation_labelset0 = self._label0_folds[self._current_fold]\n",
    "            training_imgset1 = np.concatenate( [ fold for i,fold in enumerate(self._image1_folds) if i != self._current_fold] )\n",
    "            training_labelset1 = np.concatenate( [ fold for i,fold in enumerate(self._label1_folds) if i != self._current_fold] )\n",
    "            validation_imgset1 = self._image1_folds[self._current_fold]\n",
    "            validation_labelset1 = self._label1_folds[self._current_fold]\n",
    "            \n",
    "            self._Training = SubSet(training_imgset0[:20], training_imgset1[:20], training_labelset0[:20], training_labelset1[:20])\n",
    "            self._Validation = SubSet(validation_imgset0[:20], validation_imgset1[:20], validation_labelset0[:20], validation_labelset1[:20])\n",
    "            print('Creating folds...done.')\n",
    "\n",
    "            if normalize:\n",
    "                print('Computing mean...')\n",
    "                mean = online_flattened_mean(self._Training.images)\n",
    "                print('Computing mean...done.')\n",
    "\n",
    "                self.Normalization = True\n",
    "                self._Training.Normalization = True\n",
    "                self._Validation.Normalization = True\n",
    "                self._Training.setNormalizationParameters(mean)\n",
    "                self._Validation.setNormalizationParameters(mean)\n",
    "            else:\n",
    "                self.Normalization = False\n",
    "                self._Training.Normalization = False\n",
    "                self._Validation.Normalization = False\n",
    "\n",
    "        print('Init Dataset...done.')\n",
    "\n",
    "    def next_fold(self):\n",
    "        self._current_fold += 1\n",
    "        \n",
    "        training_imgset0 = np.concatenate( [ fold for i,fold in enumerate(self._image0_folds) if i != self._current_fold] )\n",
    "        training_labelset0 = np.concatenate( [ fold for i,fold in enumerate(self._label0_folds) if i != self._current_fold] )\n",
    "        validation_imgset0 = self._image0_folds[self._current_fold]\n",
    "        validation_labelset0 = self._label0_folds[self._current_fold]\n",
    "        training_imgset1 = np.concatenate( [ fold for i,fold in enumerate(self._image1_folds) if i != self._current_fold] )\n",
    "        training_labelset1 = np.concatenate( [ fold for i,fold in enumerate(self._label1_folds) if i != self._current_fold] )\n",
    "        validation_imgset1 = self._image1_folds[self._current_fold]\n",
    "        validation_labelset1 = self._label1_folds[self._current_fold]\n",
    "\n",
    "        self._Training = SubSet(training_imgset0, training_imgset1, training_labelset0, training_labelset1)\n",
    "        self._Validation = SubSet(validation_imgset0, validation_imgset1, validation_labelset0, validation_labelset1)\n",
    "\n",
    "\n",
    "        if self.Normalization:\n",
    "            print('Computing mean and std image...')\n",
    "            mean = online_flattened_mean(self._Training.images)\n",
    "            print('Computing mean and std image...done.')\n",
    "\n",
    "            self._Training.Normalization = True\n",
    "            self._Validation.Normalization = True\n",
    "            self._Training.setNormalizationParameters(mean)\n",
    "            self._Validation.setNormalizationParameters(mean)\n",
    "        else:\n",
    "            self._Training.Normalization = False\n",
    "            self._Validation.Normalization = False\n",
    "\n",
    "        return self._Training, self._Validation\n",
    "\n",
    "\n",
    "    @property\n",
    "    def Training(self):\n",
    "        return self._Training\n",
    "\n",
    "    @property\n",
    "    def Validation(self):\n",
    "        return self._Validation\n",
    "\n",
    "    @property\n",
    "    def Test(self):\n",
    "        return self._Test\n",
    "\n",
    "class SubSet(object):\n",
    "    \"\"\"\n",
    "    Utility class to handle training and validation set structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, images0, images1, labels0, labels1):\n",
    "        \"\"\"\n",
    "        Builds dataset with images and labels.\n",
    "        Args:\n",
    "            images0: Images data of class0.\n",
    "            labels0: Labels data of class0.\n",
    "        \"\"\"\n",
    "        assert images0.shape[0] == labels0.shape[0], (\n",
    "            \"images.shape: {0}, labels.shape: {1}\".format(str(images0.shape), str(labels0.shape)))\n",
    "        assert images1.shape[0] == labels1.shape[0], (\n",
    "            \"images.shape: {0}, labels.shape: {1}\".format(str(images1.shape), str(labels1.shape)))\n",
    "\n",
    "        self._num_examples = images0.shape[0]\n",
    "        self._images = np.concatenate( (images0, images1) )\n",
    "        self._labels = np.concatenate( (labels0, labels1) )\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        \n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        \n",
    "        self._images0 = images0[perm]\n",
    "        self._labels0 = labels0[perm]\n",
    "        \n",
    "        self._images1 = images1[perm]\n",
    "        self._labels1 = labels1[perm]\n",
    "        \n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    @property\n",
    "    def index_in_epoch(self):\n",
    "        return self._index_in_epoch\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "    \n",
    "    def setNormalizationParameters(self, mean):\n",
    "        self._mean = mean\n",
    "\n",
    "    def next_batch(self, batch_size, bases3d=False):\n",
    "        \"\"\"\n",
    "        Return the next `batch_size` examples from this data set.\n",
    "        Args:\n",
    "            batch_size: Batch size.\n",
    "        \"\"\"\n",
    "        assert batch_size <= 2*self._num_examples\n",
    "        \n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += int(batch_size/2)\n",
    "        if self._index_in_epoch >= self._num_examples:\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images0 = self._images0[perm]\n",
    "            self._labels0 = self._labels0[perm]\n",
    "            self._images1 = self._images1[perm]\n",
    "            self._labels1 = self._labels1[perm]\n",
    "\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "\n",
    "\n",
    "        end = self._index_in_epoch\n",
    "\n",
    "        perm = np.arange(batch_size)\n",
    "        np.random.shuffle(perm)\n",
    "        \n",
    "        \n",
    "        label_batch = np.concatenate( (self._labels0[start:end], self._labels1[start:end]) )[perm]\n",
    "        image_batch = np.array(\n",
    "            [ self.getImageArray(image0) for image0 in self._images0[start:end]] + \n",
    "            [ self.getImageArray(image1) for image1 in self._images1[start:end]]\n",
    "        )[perm]\n",
    "#       image_batch = np.array([ np.zeros((30,512,512)) if t[0]==0 else np.ones((30,512,512)) for t in label_batch])\n",
    "        \n",
    "        image_batch = np.swapaxes(image_batch, 1, 2)\n",
    "        image_batch = np.swapaxes(image_batch, 2, 3)\n",
    "\n",
    "        # --- Only in case of 3D model ---\n",
    "        if bases3d:\n",
    "            image_batch = np.expand_dims(image_batch, axis=4)\t\t\t\n",
    "\n",
    "        return image_batch, label_batch\n",
    "\n",
    "\n",
    "    def getImageArray(self, image_path):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Numpy array of the loaded image\n",
    "        Args:\n",
    "            image_path: Path of image to read from.\n",
    "        \"\"\"\n",
    "        if self.Normalization:\n",
    "            return normalize_image(sitk.GetArrayFromImage(sitk.ReadImage(image_path))) - self._mean\n",
    "        else:\n",
    "            sl = normalize_image(sitk.GetArrayFromImage(sitk.ReadImage(image_path)))[11,:,:]\n",
    "            sl = np.expand_dims(sl, axis=0)\n",
    "            return np.repeat(sl, 3, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "datapath = r'/home/nicolab/DATA/data/SUPERVISED/REGISTRY/NCCT_THICK_RESAMPLED/'\n",
    "labelpath = r'/home/nicolab/DATA/data/Registrydatabase.xlsx'\n",
    "\n",
    "training_points, test_points = read_dataset(datapath, labelpath)\n",
    "print(len(training_points))\n",
    "print(len(test_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Dataset...\n",
      "Creating folds...\n",
      "Creating folds...done.\n",
      "Init Dataset...done.\n"
     ]
    }
   ],
   "source": [
    "dataset = DataSet(training_points, test_points, cross_validation_folds=10, normalize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "img, lab = dataset.Validation.next_batch(10)\n",
    "print(len([ x for x in dataset.Training._images0 if x in dataset.Test._images1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
